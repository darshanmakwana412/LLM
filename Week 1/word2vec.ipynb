{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec: A prediction based method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to remember our main idea for this week: We have to put information about contexts into numerical vectors\n",
    "\n",
    "While count based methods took this idea quite rigorously, Word2Vec uses it in a different manner, they try to learn word vectors by teaching them to predict contexts.\n",
    "\n",
    "Word2Vec is a model whose parameters are word vectors. These parameters are optimized iteratively for a certain objective. The objective forces word vectors to \"know\" contexts a word can appear in: the vectors are trained to predict possible contexts of the corresponding words. As you remember from the distributional hypothesis, if vectors \"know\" about contexts, they \"know\" word meaning.\n",
    "\n",
    "Word2Vec is an iterative method. Its main idea is as follows:\n",
    "\n",
    "1. take a huge text corpus;\n",
    "2. go over the text with a sliding window, moving one word at a time. At each step, there is a central word and context words (other words in this window);\n",
    "3. for the central word, compute probabilities of context words;\n",
    "4. adjust the vectors to increase these probabilities."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective Function: Negative Log Likelihood"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each position $t=1,...,T$ in a text corpus, Word2Vec predicts context words within a m-sized window given the central word $w_{t}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Likelihood = L(\\theta) = \\prod_{t=1}^{T} \\prod_{-m\\le j \\le m,j\\neq 0} P(w_{t+j}| w_{t}, \\theta)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\theta$ are all variables to be optimized. The objective function (aka loss function or cost function) $J(\\theta)$ is the average negative log-likelihood:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Loss = J(\\theta) = -\\frac{1}{T}log{L(\\theta)} = -\\frac{1}{T}\\sum_{t=1}^{T} \\sum_{-m\\le j \\le m,j\\neq 0} log{P(w_{t+j}| w_{t}, \\theta)}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In simpler terms our objective function goes over the text with a sliding window and computes the probabilities of the context words given the central word"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we calculate $P(w_{t+j}|w_{j}, \\theta)$?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $w$ each word we will have two vectors:\n",
    "* when it is a central word;\n",
    "* when it is a context word.\n",
    "\n",
    "(Once the vectors are trained, usually we throw away context vectors and use only word vectors.)\n",
    "\n",
    "Then for the central word $c$ (c - central) and the context word $o$ (o - outside word) probability of the context word is"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$P(o, c) = \\frac{exp(u_{o}^{T}v_{c})}{\\sum_{w \\in V} exp(u_{w}^{T}v_{c})}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot product measures the similarity of $o$ and $c$, the larger the dot product the greater the similarity and then we are just normalizing it to get a probability distribution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very much similar to the softmax function\n",
    "$$softmax(x_{i}) = \\frac{exp(x_{i})}{\\sum_{j=1}^{n} exp(x_{j})}$$\n",
    "It is called softmax beacause\n",
    "* \"soft\" all probabilities are non zero\n",
    "* \"max\" max $x_{i}$ has the maximum probability $p_{i}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training by Gradient Descent, one word at a time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us recall that our parameters $\\theta$ are vectors $v_{w}$ and $u_{w}$ for all words in the vocabulary. These vectors are learned by optimizing the training objective via gradient descent (with some learning rate $\\alpha$):"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\theta^{new} = \\theta^{old} - \\alpha \\nabla_{\\theta}J(\\theta)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make these updates one at a time: each update is for a single pair of a center word and one of its context words. Look again at the loss function:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Loss = J(\\theta) = -\\frac{1}{T}log{L(\\theta)} = -\\frac{1}{T}\\sum_{t=1}^{T} \\sum_{-m\\le j \\le m,j\\neq 0} log{P(w_{t+j}| w_{t}, \\theta)}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the center word $w_{t}$, the loss contains a distinct term $J_{t,j}(\\theta) = -log{P(w_{t+j}|w_{t}, \\theta)}$ for each of its context words $w_{t+j}$. Let us look in more detail at just this one term and try to understand how to make an update for this step."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$J_{t,j}(\\theta) = -log{P(w_{t+j}|w_{t}, \\theta)} = -log{\\frac{exp(u_{j}^{T}v_{t})}{\\sum_{w \\in V_{oc}} exp(u_{j}^{T}v_{t})}} = -u_{j}^{T}v_{t} + log{\\sum_{w \\in V_{oc}} exp(u_{j}^{T}v_{t})}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note which parameters are present at this step:\n",
    "* from vectors for central words, only \n",
    "* from vectors for context words, all \n",
    "\n",
    "Only these parameters will be updated at the current step.\n",
    "Let us now calculate their gradients"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$v_{t} := v_{t} - \\alpha \\frac{\\partial J_{t,j}(\\theta)}{\\partial v_{t}}$$\n",
    "$$u_{j} := u_{j} - \\alpha \\frac{\\partial J_{t,j}(\\theta)}{\\partial u_{j}} \\forall w \\in V_{oc}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Week we will be implementing the word2vec in pytorch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you find anything hard to understand please look at the paper: https://arxiv.org/pdf/1411.2738.pdf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources:\n",
    "\n",
    "* https://web.stanford.edu/class/cs224n/readings/cs224n_winter2023_lecture1_notes_draft.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
