{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way machine learning models \"see\" data is different from how we (humans) do. For example, we can easily understand the text \"I saw a cat\", but our models can not - they need vectors of features. Such vectors, or word embeddings, are representations of words which can be fed into your model.\n",
    "\n",
    "In practice, you have a vocabulary of allowed words; you choose this vocabulary in advance. For each vocabulary word, a look-up table contains its embedding. This embedding can be found using the word index in the vocabulary (i.e., you to look up the embedding in the table using word index).\n",
    "\n",
    "\n",
    "To account for unknown words (the ones which are not in the vocabulary), usually a vocabulary contains a special token \"UNK\". Alternatively, unknown tokens can be ignored or assigned a zero vector.\n",
    "\n",
    "The main objective of this week is: how do we get these word vectors?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representation as Discrete Symbols"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot vectors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most simplest way of representing words in numerical data is to represent them as one hot vectors i.e for the ith word in the vocabulary, the vector has 1 on the ith dimension and 0 on the rest. This is also the most simplest way of represting categorical data in machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the cat is very grumpy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       " [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = [\"the\", \"cat\", \"is\", \"dog\", \"grumpy\", \"nice\", \"sweet\", \"table\", \"corn\",\"<UNK>\"]\n",
    "\n",
    "def convert_to_one_hot_vector(sentence):\n",
    "    vectors = []\n",
    "    for word in sentence.split(\" \"):\n",
    "        vector = [0] * len(vocab)\n",
    "        try:\n",
    "            idx = vocab.index(word)\n",
    "        except:\n",
    "            idx = len(vocab) - 1\n",
    "        vector[idx] = 1\n",
    "        vectors.append(vector)\n",
    "    return vectors\n",
    "\n",
    "sentence = \"the cat is very grumpy\"\n",
    "embeddings = convert_to_one_hot_vector(sentence)\n",
    "\n",
    "print(sentence)\n",
    "embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the reason one hot vectors are not very popular in representing words is because for very large vocabularies, these vectors become very long and end up taking redundant space, vector dimensionality is equal to the vocabulary size\n",
    "\n",
    "But the worst part about them is that these vectors know nothing about the words they represent. As you can see \"the\" is as close to \"cat\" than to \"is\". They do not capture the underlying meaning\n",
    "\n",
    "How do we represent words so as to capture meaning?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributional Semantics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To represent meaning let us first define the notion of what \"meaning\" is that we want to represent\n",
    "\n",
    "consider the sentence: \"the ___ is very grumpy\"\n",
    "\n",
    "What all words does your brain think can fit into the context of this sentence? Maybe a \"cat\" or a \"dog\" but definitely not a \"table\" \n",
    "\n",
    "The hypothesis is that your brain searched for other words that can be used in the same contexts, found some (e.g., dog, cat), and made a conclusion that cat has meaning similar to those other words(They both are grumpy). This is the distributional hypothesis:\n",
    "\n",
    "Words which frequently appear in similar contexts have similar meaning.\n",
    "\n",
    "Often you can find it formulated as \"You shall know a word by the company it keeps\" with the reference to J. R. Firth in 1957, but actually there were a lot more people responsible, and much earlier. For example, Harris, 1954.\n",
    "\n",
    "This is an extremely valuable idea: it can be used in practice to make word vectors capture their meaning. According to the distributional hypothesis, \"to capture meaning\" and \"to capture contexts\" are inherently the same. Therefore, all we need to do is to put information about word contexts into word representation.\n",
    "\n",
    "Main idea: We need to put information about word contexts into word representation.\n",
    "\n",
    "All we'll be doing this week is looking at different ways to do this."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count Based Methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our main idea here is: We have to put information about contexts into word vectors.\n",
    "\n",
    "Count-based methods take this idea quite literally, Put this information manually based on global corpus statistics.\n",
    "\n",
    "The general procedure consists of the two steps:\n",
    "(1) construct a word-context matrix,\n",
    "(2) reduce its dimensionality.\n",
    "There are two reasons to reduce dimensionality. First, a raw matrix is very large. Second, since a lot of words appear in only a few of possible contexts, this matrix potentially has a lot of uninformative elements (e.g., zeros).\n",
    "\n",
    "To estimate similarity between words/contexts, usually you need to evaluate the dot-product of normalized word/context vectors (i.e., cosine similarity).\n",
    "\n",
    "To define a count-based method, we need to define two things:\n",
    "possible contexts (including what does it mean that a word appears in a context),\n",
    "the notion of association, i.e., formulas for computing matrix elements."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Co Occurence Counts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest approach is to define contexts as each word in an L-sized window. Matrix element for a word-context pair (w, c) is the number of times w appears in context c. This is the very basic (and very, very old) method for obtaining embeddings.\n",
    "\n",
    "Context: Surrounding words in an L sized window\n",
    "\n",
    "Matrix Element: N(w, c) number of time words w appears in c\n",
    "\n",
    "The [old grumpy cat started playing] in my room\n",
    "the context for cat is: [old grumpy cat started playing]\n",
    "the matrix element is: 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting Question: Are context words at different distances equally important? If not, how can we modify co-occurrence counts?\n",
    "\n",
    "HAL Model: https://link.springer.com/content/pdf/10.3758/BF03204766.pdf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positive Pointwise Mutual Information(PPMI)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here contexts are defined as before, but the measure of the association between word and context is more clever: positive PMI (or PPMI for short). PPMI measure is widely regarded as state-of-the-art for pre-neural distributional-similarity models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context: Surrounding words in an L sized window\n",
    "\n",
    "Matrix Element:\n",
    "\n",
    "$$PPMI(w, c) = max(0, PMI(w, c))$$\n",
    "$$PMI(w, c) = \\log{\\frac{P(w, c)}{P(w)P(c)}} = \\log{\\frac{N(w, c)|(w,c)|}{N(w)N(c)}}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent Semantic Analysis (LSA) analyzes a collection of documents. While in the previous approaches contexts served only to get word vectors and were thrown away afterward, here we are also interested in context, or, in this case, document vectors. LSA is one of the simplest topic models: cosine similarity between document vectors can be used to measure similarity between documents.\n",
    "\n",
    "The term \"LSA\" sometimes refers to a more general approach of applying SVD to a term-document matrix where the term-document elements can be computed in different ways (e.g., simple co-occurrence, tf-idf, or some other weighting)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "context: document d (from a collection D)\n",
    "\n",
    "Matix Element:\n",
    "$$tf_idf(w, d, D) = tf(w, d)*idf(w, D)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources:\n",
    "\n",
    "* http://lsa.colorado.edu/papers/JASIS.lsi.90.pdf\n",
    "* https://en.wikipedia.org/wiki/Latent_semantic_analysis\n",
    "* https://en.wikipedia.org/wiki/Tf%E2%80%93idf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
